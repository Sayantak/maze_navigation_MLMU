# @package _global_

# Planning configuration
planning:
  # Split index configuration: "random" or an integer
  split_idx_mode: "random"  # Position to split the sequence for planning
  cut_outputs: True # discard the prompt when passing it to the adapter
  
  # Sampling parameters
  num_samples: 5       # Number of samples to generate during planning
  continuation_length: 10  # Length of continuations to sample
  
  # Embedding initialization
  use_gpt_embeddings: False  # Whether to initialize TransformerSampleEncoder with GPT2 embeddings
  
  # TokenSampleAdapter configuration
  adapter:
    projection_dim: null  # If null, uses hidden_size
    use_aggregator_layer_norm: False  # Apply layer norm in aggregator
    disable_aggregator_transformer: False  # Disable transformer in aggregator
    use_encoder_layer_norm: False  # Apply layer norm in encoder
    encoder_num_layers: 1  # Number of layers in encoder
    aggregator_num_layers: 1  # Number of layers in aggregator
    disable_attention_mask: False  # Disable attention mask
    use_cls_token: False  # Use CLS token
  
  # Add any other planner-specific parameters here in the future
  # For example:
  # temperature: 0.7
  # top_p: 0.9
  # use_beam_search: False 