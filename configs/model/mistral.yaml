# @package _global_
model_name : mistral

model:
  _target_: recipe.models.MistralPL
  train_mode: ar
  from_pretrained: True # False is not allowed, this is trained with lora
  lora_r_alpha: 256

datamodule:
  tokenizer: mistralai/Mistral-7B-v0.1
  batch_size: 8

optim:
  learning_rate: 1e-3

trainer:
  accumulate_grad_batches: 8
